import os
import re
from time import sleep

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import ElementNotInteractableException
from selenium.common.exceptions import StaleElementReferenceException
from bs4 import BeautifulSoup

import csv
import ssl
from urllib.request import urlopen
from urllib.parse import quote_plus

options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument('lang=ko_KR')
chromedriver_path = "/chromdriver/chromedriver"
driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver Ïó¥Í∏∞


def main():
    global driver, load_wb, review_num

    driver.implicitly_wait(4)  # Î†åÎçîÎßÅ Îê†ÎïåÍπåÏßÄ Í∏∞Îã§Î¶∞Îã§ 4Ï¥à
    driver.get('https://map.kakao.com/')  # Ï£ºÏÜå Í∞ÄÏ†∏Ïò§Í∏∞

    # Í≤ÄÏÉâÌï† Î™©Î°ù
    place_infos = ['ÎßõÏßë']

    for i, place in enumerate(place_infos):
        # delay
        if i % 4 == 0 and i != 0:
            sleep(5)
        print("#####", i)
        search(place)

    driver.quit()
    print("finish")


def search(place):
    global driver

    search_area = driver.find_element_by_xpath('//*[@id="search.keyword.query"]')  # Í≤ÄÏÉâ Ï∞Ω
    search_area.send_keys(place)  # Í≤ÄÏÉâÏñ¥ ÏûÖÎ†•
    driver.find_element_by_xpath('//*[@id="search.keyword.submit"]').send_keys(Keys.ENTER)  # EnterÎ°ú Í≤ÄÏÉâ
    sleep(2)

    # Í≤ÄÏÉâÎêú Ï†ïÎ≥¥Í∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎßå ÌÉêÏÉâ
    # 1Î≤à ÌéòÏù¥ÏßÄ place list ÏùΩÍ∏∞
    html = driver.page_source

    soup = BeautifulSoup(html, 'html.parser')
    place_lists = soup.select('.placelist > li')  # Í≤ÄÏÉâÎêú Ïû•ÏÜå Î™©Î°ù
    # place_lists = soup.select('.placelist') # Í≤ÄÏÉâÎêú Ïû•ÏÜå Î™©Î°ù

    # Í≤ÄÏÉâÎêú Ï≤´ ÌéòÏù¥ÏßÄ Ïû•ÏÜå Î™©Î°ù ÌÅ¨Î°§ÎßÅÌïòÍ∏∞
    crawling(place, place_lists)
    search_area.clear()

    # Ïö∞ÏÑ† ÎçîÎ≥¥Í∏∞ ÌÅ¥Î¶≠Ìï¥ÏÑú 2ÌéòÏù¥ÏßÄ
    try:
        driver.find_element_by_xpath('//*[@id="info.search.place.more"]').send_keys(Keys.ENTER)     # ÎçîÎ≥¥Í∏∞ ÌÅ¥Î¶≠ Î∂ÄÎ∂Ñ
        sleep(2)

        # 1~ 5ÌéòÏù¥ÏßÄ ÏùΩÍ∏∞
        Search(place)

    except ElementNotInteractableException:
        print('not found')

    try:
        driver.find_element_by_xpath('//*[@id="info.search.page.next"]').send_keys(Keys.ENTER)     # ÌéòÏù¥ÏßÄ ÎÑòÍ∏∞Í∏∞(>Î≤ÑÌäº)
        sleep(2)
        # 6ÌéòÏù¥ÏßÄ Ïù¥ÌõÑ ÏùΩÍ∏∞
        crawling(place, place_lists)
        search_area.clear()
        Search(place)

    except ElementNotInteractableException:
        print('not found')

    finally:
        search_area.clear()


def Search(place):      # ÌéòÏù¥ÏßÄÏùΩÍ∏∞

    for i in range(2, 6):
        # ÌéòÏù¥ÏßÄ ÎÑòÍ∏∞Í∏∞
        xPath = '//*[@id="info.search.page.no' + str(i) + '"]'  # Í∞Å ÌéòÏù¥ÏßÄÎ≤àÌò∏Ïùò Ïà´Ïûê(Î≤ÑÌäº)ÏúºÎ°ú Ïù¥Îèô
        driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)  # Ïà´ÏûêÎ≤ÑÌäº ÎàÑÎ•¥Í∏∞
        sleep(2)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        place_lists = soup.select('.placelist > li')  # Ïû•ÏÜå Î™©Î°ù list
        # place_lists = soup.select('.placelist') # Ïû•ÏÜå Î™©Î°ù list
        crawling(place, place_lists)


def crawling(place, place_lists):
    """
    ÌéòÏù¥ÏßÄ Î™©Î°ùÏùÑ Î∞õÏïÑÏÑú ÌÅ¨Î°§ÎßÅ ÌïòÎäî Ìï®Ïàò
    :param place: Î¶¨Î∑∞ Ï†ïÎ≥¥ Ï∞æÏùÑ Ïû•ÏÜåÏù¥Î¶Ñ
    """

    ad_flg = 0

    while_flag = False
    for i, place in enumerate(place_lists):
        # Í¥ëÍ≥†Ïóê Îî∞ÎùºÏÑú index Ï°∞Ï†ïÌï¥ÏïºÌï®
        # if i >= 6:
        i = i + ad_flg;

        try:
            detail_page_xpath = '//*[@id="info.search.place.list"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'
            driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)
            driver.switch_to.window(driver.window_handles[-1])  # ÏÉÅÏÑ∏Ï†ïÎ≥¥ ÌÉ≠ÏúºÎ°ú Î≥ÄÌôò
            sleep(1)
            place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name
            place_address = place.select('.info_item > .addr > p')[0].text  # place address
            place_rate = place.select('.rating > .score > em')[0].text
            print('####', place_name)
            print('####', place_address)
            print('####', place_rate)
            # Ï≤´ ÌéòÏù¥ÏßÄ
            extract_review(place_name, place_address,place_rate)

            # 2-5 ÌéòÏù¥ÏßÄ
            idx = 3
            try:
                page_num = len(driver.find_elements_by_class_name('link_page'))  # ÌéòÏù¥ÏßÄ Ïàò Ï∞æÍ∏∞
                for i in range(page_num-1):
                    # css selectorÎ•º Ïù¥Ïö©Ìï¥ ÌéòÏù¥ÏßÄ Î≤ÑÌäº ÎàÑÎ•¥Í∏∞
                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)
                    sleep(1)
                    extract_review(place_name, place_address,place_rate)
                    idx += 1
                driver.find_element_by_link_text('Îã§Ïùå').send_keys(Keys.ENTER)  # 5ÌéòÏù¥ÏßÄÍ∞Ä ÎÑòÎäî Í≤ΩÏö∞ Îã§Ïùå Î≤ÑÌäº ÎàÑÎ•¥Í∏∞
                sleep(1)
                extract_review(place_name, place_address,place_rate)  # Î¶¨Î∑∞ Ï∂îÏ∂ú
            except (NoSuchElementException, ElementNotInteractableException):
                print("no review in crawling 1")

            # Í∑∏ Ïù¥ÌõÑ ÌéòÏù¥ÏßÄ
            while True:
                idx = 5
                try:
                    page_num = len(driver.find_elements_by_class_name('link_page'))
                    for i in range(page_num - 1):
                        driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)
                        sleep(1)
                        extract_review(place_name, place_address,place_rate)
                        idx += 1
                    driver.find_element_by_link_text('Îã§Ïùå').send_keys(Keys.ENTER)  # 10ÌéòÏù¥ÏßÄ Ïù¥ÏÉÅÏúºÎ°ú ÎÑòÏñ¥Í∞ÄÍ∏∞ ÏúÑÌïú Îã§Ïùå Î≤ÑÌäº ÌÅ¥Î¶≠
                    sleep(1)
                    extract_review(place_name, place_address,place_rate)  # Î¶¨Î∑∞ Ï∂îÏ∂ú
                except (NoSuchElementException, ElementNotInteractableException):
                    print("no review in crawling 2")
                    break

            driver.close()
            driver.switch_to.window(driver.window_handles[0])  # Í≤ÄÏÉâ ÌÉ≠ÏúºÎ°ú Ï†ÑÌôò

        except (NoSuchElementException, ElementNotInteractableException):
            print("Ad Item")


def clean_text(text):
    # content = text.get_text()
    content = text
    # cleaned_text = re.sub('[a-zA-Z]', '', content)
    cleaned_text = re.sub('[,]', ' ', content)
    # cleaned_text = cleaned_text.replace("üá≤\u200büáÆ\u200büá±\u200büá±\u200büáÆ\u200büá™\u200b", "")
    # cleaned_text = cleaned_text.replace("Ïò§Î•òÎ•º Ïö∞ÌöåÌïòÍ∏∞ ÏúÑÌïú Ìï®Ïàò Ï∂îÍ∞Ä ", "")
    # cleaned_text = cleaned_text.replace("ÎèôÏòÅÏÉÅ Îâ¥Ïä§ Ïò§Î•òÎ•º Ïö∞ÌöåÌïòÍ∏∞ ÏúÑÌïú Ìï®Ïàò Ï∂îÍ∞Ä ", "")
    # cleaned_text = cleaned_text.replace("Î¨¥Îã®Ï†ÑÏû¨ Î∞è Ïû¨Î∞∞Ìè¨ Í∏àÏßÄ", "")
    # cleaned_text = cleaned_text.strip('\n')
    return cleaned_text


def extract_review(place_name, place_address, place_rate):
    global driver

    ret = True

    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    # more_xpath = ('/html/body/div[2]/div[3]/div[2]/div[4]/div[4]/ul/li[1]/div[2]/p/button')   # ÎåìÍ∏ÄÏóê ÎçîÎ≥¥Í∏∞ Î≤ÑÌäº
    # more_button = driver.find_element_by_xpath(more_xpath).send_keys(Keys.ENTER)
    # Ï≤´ ÌéòÏù¥ÏßÄ Î¶¨Î∑∞ Î™©Î°ù Ï∞æÍ∏∞
    review_lists = soup.select('.list_evaluation > li')

    # Î¶¨Î∑∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞
    if len(review_lists) != 0:
        for i, review in enumerate(review_lists):
            comment = review.select('.txt_comment > span')  # Î¶¨Î∑∞
            rating = review.select('.grade_star > em')  # Î≥ÑÏ†ê
            val = ''
            if len(comment) != 0:
                if len(rating) != 0:
                    # if len(review.select('.txt_fold')) > 0:
                    #     # more_click()
                    #     driver.find_element_by_xpath('/html/body/div[2]/div[3]/div[2]/div[5]/div[4]/ul/li[1]/div[2]/p/button').send_keys(Keys.ENTER)
                    #     print("Working")
                    #     comment1 = driver.find_element_by_xpath('//*[@id="mArticle"]/div[5]/div[4]/ul/li[1]/div[2]/p/span')
                    #     #comment = comment1.se;('span')
                    #     sleep(3)
                    #     print(comment1.get_attribute('text'))
                    val = place_name + ',' + place_address + ',' + place_rate + ',' + clean_text(comment[0].text) + ',' + rating[0].text.replace('Ï†ê', '')
                else:
                    val = place_name + ',' + place_address + ',' + place_rate + ',' + clean_text(comment[0].text) + ',' + '/0'
                print(val)
                with open('ÎßõÏßë.csv', 'a', encoding='utf-8', newline='')as writer_csv:
                    writer = csv.writer(writer_csv, delimiter=',')
                    writer.writerow([val])
    else:
        print('no review in extract')
        ret = False
    return ret


if __name__ == "__main__":
    main()
